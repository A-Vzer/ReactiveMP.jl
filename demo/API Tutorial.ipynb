{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling ReactiveMP [a194aa59-28ba-4574-a09c-4a745416d6e3]\n",
      "└ @ Base loading.jl:1342\n"
     ]
    }
   ],
   "source": [
    "# Reactive programming layer for Julia\n",
    "using Rocket \n",
    "# Core package for Constrained Bethe Free Energy minimsation with Factor graphs and message passing\n",
    "using ReactiveMP \n",
    "# High-level user friendly probabilistic model and constraints specification language for ReactiveMP\n",
    "using GraphPPL\n",
    "# Optionally include Distributions.jl and Random from Base\n",
    "using Distributions, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook covers fundamentals of ReactiveMP.jl, for advanced usage we refer to the documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General syntax for model creation\n",
    "\n",
    "We use `@model` macro from `GraphPPL.jl` package to create a probabilistic model $p(s, y)$ and also specify extra constraints on variational family of distributions $\\mathcal{Q}$.\n",
    "Below there is a simple example of general syntax for model creation. In this tutorial we do not cover all possible way to create models or extra features of `GraphPPL.jl` and we refer a reader to the documentation for more rigorous explanations and examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_model1 (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `@model` macro accepts a regular Julia function\n",
    "@model function test_model1(s_mean, s_precision)\n",
    "    \n",
    "    # We use `randomvar` function to create \n",
    "    # random variables in our model\n",
    "    s = randomvar()\n",
    "    \n",
    "    # `tilde` expression creates a functional dependencies\n",
    "    # between variables in our model and can be read as \n",
    "    # `sampled from`\n",
    "    s ~ GaussianMeanPrecision(s_mean, s_precision)\n",
    "    \n",
    "    # We use `datavar` function to create \n",
    "    # observed data variables in our models\n",
    "    # We also need to specify the type of our data \n",
    "    # In this example it is `Float64`\n",
    "    y = datavar(Float64)\n",
    "    \n",
    "    y ~ GaussianMeanPrecision(s, 1.0)\n",
    "    \n",
    "    return s, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@model` macro creates a function with the same name and with the same set of input arguments as the original function (`test_model1(s_mean, s_precision)` in this example). However, the return value is modified in such a way to contain a reference to the model object as the first value and user specified variables in a form of a tuple as the second value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (s, y) = test_model1(0.0, 1.0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later on we can examine our model structure with the help of some utility functions such as: \n",
    "- `getnodes()`: returns an array of factor nodes in a correposning factor graph\n",
    "- `getrandom()`: returns an array of random variable in the model\n",
    "- `getdata()`: returns an array of data inputs in the model\n",
    "- `getconstant()`: return an array of constant values in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{ReactiveMP.AbstractFactorNode}:\n",
       " FactorNode:\n",
       " form            : NormalMeanPrecision\n",
       " sdtype          : Stochastic()\n",
       " interfaces      : (Interface(out, Marginalisation()), Interface(μ, Marginalisation()), Interface(τ, Marginalisation()))\n",
       " factorisation   : ((1, 2, 3),)\n",
       " local marginals : (:out_μ_τ,)\n",
       " metadata        : nothing\n",
       " pipeline        : FactorNodePipeline(functional_dependencies = DefaultFunctionalDependencies(), extra_stages = EmptyPipelineStage()\n",
       "\n",
       " FactorNode:\n",
       " form            : NormalMeanPrecision\n",
       " sdtype          : Stochastic()\n",
       " interfaces      : (Interface(out, Marginalisation()), Interface(μ, Marginalisation()), Interface(τ, Marginalisation()))\n",
       " factorisation   : ((1, 2, 3),)\n",
       " local marginals : (:out_μ_τ,)\n",
       " metadata        : nothing\n",
       " pipeline        : FactorNodePipeline(functional_dependencies = DefaultFunctionalDependencies(), extra_stages = EmptyPipelineStage()\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getnodes(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Symbol}:\n",
       " :s"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getrandom(model) .|> name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Symbol}:\n",
       " :y"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getdata(model) .|> name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 0.0\n",
       " 1.0\n",
       " 1.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getconstant(model) .|> getconst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use control flow statements in model specification such as `if` or `for` blocks. In principal, any valid Julia code can be used inside `@model` block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_model2 (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function test_model2(n)\n",
    "    \n",
    "    if n <= 1\n",
    "        error(\"`n` argument must be greater than one.\")\n",
    "    end\n",
    "    \n",
    "    # `randomvar(n)` creates a dense sequence of \n",
    "    # random variables\n",
    "    s = randomvar(n)\n",
    "    \n",
    "    # `datavar(Float64, n)` creates a dense sequence of \n",
    "    # observed data variables of type `Float64`\n",
    "    y = datavar(Float64, n)\n",
    "    \n",
    "    s[1] ~ GaussianMeanPrecision(0.0, 0.1)\n",
    "    y[1] ~ GaussianMeanPrecision(s[1], 1.0)\n",
    "    \n",
    "    for i in 2:n\n",
    "        s[i] ~ GaussianMeanPrecision(s[i - 1], 1.0)\n",
    "        y[i] ~ GaussianMeanPrecision(s[i], 1.0)\n",
    "    end\n",
    "    \n",
    "    return s, y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (s, y) = test_model2(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of factor nodes in generated Factor Graph\n",
    "getnodes(model) |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of random variables\n",
    "getrandom(model) |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of data inputs\n",
    "getdata(model) |> length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An amount of constant values\n",
    "getconstant(model) |> length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use complex expression inside functional dependencies expressions\n",
    "\n",
    "```julia\n",
    "y ~ NormalMeanPrecision(2.0 * (s + 1.0), 1.0)\n",
    "```\n",
    "\n",
    "`~` operator automatically creates a random variable if none was created before with the same name and errors if this name already exists\n",
    "\n",
    "```julia\n",
    "# s = randomvar() here is optional\n",
    "# `~` creates random variables automatically\n",
    "s ~ NormalMeanPrecision(0.0, 1.0)\n",
    "```\n",
    "\n",
    "An error example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "LoadError: Invalid name 's' for new random variable. 's' was already initialized with '=' operator before.\nin expression starting at /Users/bvdmitri/.julia/dev/GraphPPL/src/GraphPPL.jl:161",
     "output_type": "error",
     "traceback": [
      "LoadError: Invalid name 's' for new random variable. 's' was already initialized with '=' operator before.\nin expression starting at /Users/bvdmitri/.julia/dev/GraphPPL/src/GraphPPL.jl:161",
      "",
      "Stacktrace:",
      "  [1] error(s::String)",
      "    @ Base ./error.jl:33",
      "  [2] (::GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol})(expression::Expr)",
      "    @ GraphPPL ~/.julia/dev/GraphPPL/src/GraphPPL.jl:302",
      "  [3] walk",
      "    @ ~/.julia/packages/MacroTools/HDyRe/src/utils.jl:112 [inlined]",
      "  [4] postwalk",
      "    @ ~/.julia/packages/MacroTools/HDyRe/src/utils.jl:122 [inlined]",
      "  [5] (::MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}})(x::Expr)",
      "    @ MacroTools ~/.julia/packages/MacroTools/HDyRe/src/utils.jl:122",
      "  [6] iterate",
      "    @ ./generator.jl:47 [inlined]",
      "  [7] collect_to!(dest::Vector{Any}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, offs::Int64, st::Int64)",
      "    @ Base ./array.jl:728",
      "  [8] collect_to!(dest::Vector{LineNumberNode}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, offs::Int64, st::Int64)",
      "    @ Base ./array.jl:736",
      "  [9] collect_to_with_first!(dest::Vector{LineNumberNode}, v1::LineNumberNode, itr::Base.Generator{Vector{Any}, MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, st::Int64)",
      "    @ Base ./array.jl:706",
      " [10] _collect(c::Vector{Any}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}}, #unused#::Base.EltypeUnknown, isz::Base.HasShape{1})",
      "    @ Base ./array.jl:700",
      " [11] collect_similar(cont::Vector{Any}, itr::Base.Generator{Vector{Any}, MacroTools.var\"#19#20\"{GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol}}})",
      "    @ Base ./array.jl:606",
      " [12] map(f::Function, A::Vector{Any})",
      "    @ Base ./abstractarray.jl:2294",
      " [13] walk(x::Expr, inner::Function, outer::GraphPPL.var\"#41#54\"{ReactiveMPBackend, Set{Symbol}, Set{Symbol}, Symbol})",
      "    @ MacroTools ~/.julia/packages/MacroTools/HDyRe/src/utils.jl:112",
      " [14] postwalk(f::Function, x::Expr)",
      "    @ MacroTools ~/.julia/packages/MacroTools/HDyRe/src/utils.jl:122",
      " [15] generate_model_expression(backend::ReactiveMPBackend, model_options::Expr, model_specification::Expr)",
      "    @ GraphPPL ~/.julia/dev/GraphPPL/src/GraphPPL.jl:272",
      " [16] var\"@model\"(__source__::LineNumberNode, __module__::Module, model_options::Any, model_specification::Any)",
      "    @ GraphPPL ~/.julia/dev/GraphPPL/src/GraphPPL.jl:165",
      " [17] eval",
      "    @ ./boot.jl:360 [inlined]",
      " [18] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "    @ Base ./loading.jl:1116"
     ]
    }
   ],
   "source": [
    "@model function error_model1()\n",
    "    s = 1.0\n",
    "    s ~ NormalMeanPrecision(0.0, 1.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `GraphPPL.jl` creates new references for constants (literals like `0.0` or `1.0`) in a model. In some situtations it may not be efficient especially if this constants represent some matrices. `GraphPPL.jl` will create a new copy of some constant matrix in a model every time it uses it. However it is possible to use `constvar()` function to create and reuse constant in the model specification syntax\n",
    "\n",
    "```julia\n",
    "# Creates constant reference in a model with a prespecified value\n",
    "c = constvar(0.0)\n",
    "```\n",
    "\n",
    "An example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_model5 (generic function with 1 method)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function test_model5(dim::Int, n::Int, A::Matrix, P::Matrix, Q::Matrix)\n",
    "    \n",
    "    s = randomvar(n)\n",
    "    \n",
    "    y = datavar(Vector{Float64}, n)\n",
    "    \n",
    "    # Here we create constant references\n",
    "    # for constant matrices in our model \n",
    "    # to make inference a little bit more efficient\n",
    "    cA = constvar(A)\n",
    "    cP = constvar(P)\n",
    "    cQ = constvar(Q)\n",
    "    \n",
    "    s[1] ~ MvGaussianMeanCovariance(zeros(dim), cP)\n",
    "    y[1] ~ MvGaussianMeanCovariance(s[1], cQ)\n",
    "    \n",
    "    for i in 2:n\n",
    "        s[i] ~ MvGaussianMeanCovariance(cA * s[i - 1], cP)\n",
    "        y[i] ~ MvGaussianMeanCovariance(s[i], cQ)\n",
    "    end\n",
    "    \n",
    "    return s, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`~` expression also may return a reference to a newly created node in a corresponding factor graph for better convenience or later usage:\n",
    "\n",
    "```julia\n",
    "@model function test_model()\n",
    "\n",
    "    # In this example `ynode` refers to the corresponding \n",
    "    # `GaussianMeanVariance` node created in the factor graph\n",
    "    ynode, y ~ GaussianMeanVariance(0.0, 1.0)\n",
    "    \n",
    "    return ynode, y\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in ReactiveMP.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReactiveMP.jl uses `Rocket.jl` library API for inference routines. `Rocket.jl` is a reactive programming extenstions library for Julia that is higly inspired by `RxJS` and similar libraries from `Rx` ecosystem. It consists of **observables**, **actors**, **subscriptions** and **operators**. For more infromation and rigorous examples see [Rocket.jl github page](https://github.com/biaslab/Rocket.jl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observables\n",
    "Observables are lazy push-based collections and deliver their values over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerObservable(1000, 1000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Timer that emits a new value every second and has an initial one second delay \n",
    "observable = timer(1000, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subscription allows to subscribe on future values in observable and actors specify what to do with new values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerSubscription()"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "actor = (value) -> println(value)\n",
    "subscription1 = subscribe!(observable, actor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We always need to unsubscribe from some observables\n",
    "unsubscribe!(subscription1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProxyObservable(Int64, MapProxy(Int64))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can modify our observables\n",
    "modified = observable |> filter(d -> rem(d, 2) === 1) |> map(Int, d -> d ^ 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimerSubscription()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "9\n",
      "25\n",
      "49\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "subscription2 = subscribe!(modified, (value) -> println(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsubscribe!(subscription2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReactiveMP.jl` library returns posterior marginals in a form of an observable. It is possible to subscribe on its future updates, but for convenience `ReactiveMP.jl` caches last obtained value of all marginals in a model. To get a reference for the posterior marginal of some random variable in a model `ReactiveMP.jl` exports two functions: \n",
    "- `getmarginal(x)`: for a single random variable `x`\n",
    "- `getmarginals(xs)`: for a dense sequence of random variables `sx`\n",
    "\n",
    "Lets see how it works in practice. Here we create a simple coin toss model. We assume that observations are governed by the `Bernoulli` distribtuion with unknown bias parameter `θ`. To have a fully Bayesian treatment of this problem we endow `θ` with the `Beta` prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coin_toss_model (generic function with 1 method)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function coin_toss_model(n)\n",
    "\n",
    "    # `datavar` creates data 'inputs' in our model\n",
    "    # We will pass data later on to these inputs\n",
    "    # In this example we create a sequence of inputs that accepts Float64\n",
    "    y = datavar(Float64, n)\n",
    "    \n",
    "    # We endow θ parameter of our model with some prior\n",
    "    θ ~ Beta(2.0, 7.0)\n",
    "    \n",
    "    # We assume that outcome of each coin flip \n",
    "    # is governed by the Bernoulli distribution\n",
    "    for i in 1:n\n",
    "        y[i] ~ Bernoulli(θ)\n",
    "    end\n",
    "    \n",
    "    # We return references to our data inputs and θ parameter\n",
    "    # We will use these references later on during inference step\n",
    "    return y, θ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (y, θ) = coin_toss_model(500);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As soon as we have a new value for marginal posterior over `θ` variable\n",
    "# we simply print first two statistics of it\n",
    "θ_subscription = subscribe!(getmarginal(θ), (marginal) -> println(\"New update: mean(θ) = \", mean(marginal), \", std(θ) = \", std(marginal)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, lets define our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.75 # Bias of a coin\n",
    "\n",
    "dataset = float.(rand(Bernoulli(p), 500));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To pass data to our model we use `update!` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New update: mean(θ) = 0.7269155206286837, std(θ) = 0.01972901448985252\n"
     ]
    }
   ],
   "source": [
    "update!(y, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is necessary to always unsubscribe from running observables\n",
    "unsubscribe!(θ_subscription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReactiveMP.jl inference backedn is lazy and do not compute posterior marginals if no-one is listening for them\n",
    "# At this moment we already unsubscribed from new posterior updates so this `update!` does nothing\n",
    "update!(y, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rocket.jl provides some useful built-in actors for obtaining posterior marginals especially with static datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KeepActor{Marginal}(Marginal[])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `keep` actor simply keeps all incoming updates in an internal storage, ordered\n",
    "θvalues = keep(Marginal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `getmarginal` always emits last cached value as its first value\n",
    "subscribe!(getmarginal(θ) |> take(1), θvalues);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=370.0, β=139.0))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscribe!(getmarginal(θ) |> take(1), θvalues);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=370.0, β=139.0))\n",
       " Marginal(Beta{Float64}(α=370.0, β=139.0))"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BufferActor{Marginal, Vector{Marginal}}(Marginal[#undef])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `buffer` actor keeps very last incoming update in an internal storage and can also store \n",
    "# an array of updates for a sequence of random variables\n",
    "θbuffer = buffer(Marginal, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=370.0, β=139.0))"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θbuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscribe!(getmarginals([ θ ]) |> take(1), θbuffer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1-element Vector{Marginal}:\n",
       " Marginal(Beta{Float64}(α=370.0, β=139.0))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(θbuffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was an example of exact Bayesian inference with Sum-Product (or Belief Propagation) algorithm. However, ReactiveMP.jl is not limited to only sum-product algoritm but also supports variational message passing with [Constrained Bethe Free Energy Minimisation](https://www.mdpi.com/1099-4300/23/7/807)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a very high-level, ReactiveMP.jl is aimed to solve the Constrained Bethe Free Energy minimisation problem. For this task we often need to specify extra factorisation on variatonal family of distributions $q \\in \\mathcal{Q}$. For this purpose `@model` macro supports optional `where { ... }` clauses for every `~` expression in a model specification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_model6 (generic function with 1 method)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function test_model6(n)\n",
    "    τ ~ GammaShapeRate(1.0, 1.0) \n",
    "    μ ~ NormalMeanVariance(0.0, 100.0)\n",
    "    \n",
    "    y = datavar(Float64, n)\n",
    "    \n",
    "    for i in 1:n\n",
    "        # Here we assume a mean-field assumption on our \n",
    "        # variational family of distributions locally for the current node\n",
    "        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\n",
    "    end\n",
    "    \n",
    "    return μ, τ, y\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several options to specify the mean-field factorisation constraint. \n",
    "\n",
    "```julia\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) } # With names from model specification\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out)q(mean)q(precision) } # With names from node specification\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = MeanField() } # With alias name\n",
    "```\n",
    "\n",
    "It is also possible to use local structured factorisation:\n",
    "\n",
    "```julia\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i], μ)q(τ) } # With names from model specification\n",
    "y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(out, mean)q(precision) } # With names from node specification\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An an option `@model` macro accepts optional arguments for model specification, one of which is `default_factorisation` that accepts `MeanField()` as its argument for better convenience\n",
    "\n",
    "```julia\n",
    "@model [ default_factorisation = MeanField() ] function test_model(...)\n",
    "    ...\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run inference on this model we again need to create a synthetic dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = rand(Normal(-3.0, inv(sqrt(5.0))), 1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (μ, τ, y) = test_model6(length(dataset));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For variational inference we also usually need to set initial marginals for our inference procedure. For that purpose `ReactiveMP.jl` export `setmarginal!` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "setmarginal!(μ, vague(NormalMeanPrecision))\n",
    "setmarginal!(τ, vague(GammaShapeRate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "μ_values = keep(Marginal)\n",
    "τ_values = keep(Marginal)\n",
    "\n",
    "μ_subscription = subscribe!(getmarginal(μ), μ_values)\n",
    "τ_subscription = subscribe!(getmarginal(τ), τ_values)\n",
    "\n",
    "for i in 1:10\n",
    "    update!(y, dataset)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Marginal}:\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-3.0067821818264343e-9, w=0.010000001002000566))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-27.53188940920315, w=9.184909095347356))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-9542.198405846082, w=3179.91536872275))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14558.342535507441, w=4851.528446734716))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14565.984264326395, w=4854.075027036634))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14565.991894780827, w=4854.077569859672))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14565.991902396012, w=4854.077572397454))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14565.991902403606, w=4854.077572400089))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14565.99190240359, w=4854.077572400089))\n",
       " Marginal(NormalWeightedMeanPrecision{Float64}(xi=-14565.99190240359, w=4854.077572400089))"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(μ_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10-element Vector{Marginal}:\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=5.000000000046053e14))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=54605.44565548494))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=157.55185828100187))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=103.26663816710823))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=103.2124615573716))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=103.21240748910154))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=103.21240743514142))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=103.21240743508743))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=103.21240743508754))\n",
       " Marginal(GammaShapeRate{Float64}(a=501.0, b=103.21240743508754))"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(τ_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "μ: mean = -3.0007744386337576, std = 0.014353130838941412\n"
     ]
    }
   ],
   "source": [
    "println(\"μ: mean = \", mean(last(μ_values)), \", std = \", std(last(μ_values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "τ: mean = 4.854067572400048, std = 0.21686374576309103\n"
     ]
    }
   ],
   "source": [
    "println(\"τ: mean = \", mean(last(τ_values)), \", std = \", std(last(τ_values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Form constraints\n",
    "\n",
    "In order to support form constraint `randomvar()` function also supports `where { ... }` clause with some optional arguments. One of these arguments is `form_constraint` that allows to specify additional extra form constraint to random variables in our model. Another one is `prod_constraint` that allows to specify an additional constraints during computation of product of two colliding messages. For example we can perform EM algorithm if we assign a point mass contraint on some variables in our model.\n",
    "\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 50%;\" src=\"./pics/posterior.png\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "test_model7 (generic function with 1 method)"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function test_model7(n)\n",
    "    τ ~ GammaShapeRate(1.0, 1.0) \n",
    "    \n",
    "    # In case of form constraints `randomvar()` call is necessary\n",
    "    μ = randomvar() where { form_constraint = PointMassFormConstraint() }\n",
    "    μ ~ NormalMeanVariance(0.0, 100.0)\n",
    "    \n",
    "    y = datavar(Float64, n)\n",
    "    \n",
    "    for i in 1:n\n",
    "        y[i] ~ NormalMeanPrecision(μ, τ) where { q = q(y[i])q(μ)q(τ) }\n",
    "    end\n",
    "    \n",
    "    return μ, τ, y\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (μ, τ, y) = test_model7(length(dataset));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "setmarginal!(μ, vague(NormalMeanPrecision))\n",
    "setmarginal!(τ, PointMass(1.0))\n",
    "\n",
    "μ_values = keep(Marginal)\n",
    "τ_values = keep(Marginal)\n",
    "\n",
    "μ_subscription = subscribe!(getmarginal(μ), μ_values)\n",
    "τ_subscription = subscribe!(getmarginal(τ), τ_values)\n",
    "\n",
    "for i in 1:10\n",
    "    update!(y, dataset)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marginal(PointMass{Float64}(-2.9913047925590215))"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(μ_values) |> last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Marginal(GammaShapeRate{Float64}(a=501.0, b=106.01537627984227))"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getvalues(τ_values) |> last "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `ReactiveMP.jl` tries to compute an analytical product of two colliding messages and throws an error if no analytical solution is known. However, it is possible to fallback to a generic product that does not require an analytical solution to be known. In this case inference backend will simply propagate product of two message in a form of a tuple. It is not possible to use such a tuple-product during an inference and in this case it is mandatory to use some form constraint to approximate this product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "μ = randomvar() where { \n",
    "    prod_constraint = ProdGeneric(),\n",
    "    form_constraint = SampleListFormConstraint() \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it is usefull to preserve a specific parametrisation of the resulting product later on in an inference procedure. `ReactiveMP.jl` exports special `prod_constraint` called `ProdPreserveType` especially for that purpose:\n",
    "\n",
    "```julia\n",
    "μ = randomvar() where { prod_constraint = ProdPreserveType(NormalWeightedMeanPrecision) }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During variational inference `ReactiveMP.jl` optimises a special functional called Bethe Free Energy functional. It is possible to obtain its values over VMP iterations with `score` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, (μ, τ, y) = test_model6(length(dataset));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProxyObservable(Real, MapProxy(Tuple{ReactiveMP.InfCountingReal, ReactiveMP.InfCountingReal}))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bfe_observable = score(BetheFreeEnergy(), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "bfe_subscription = subscribe!(bfe_observable, (fe) -> println(\"Current BFE value: \", fe));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current BFE value: 14763.268311193242\n",
      "Current BFE value: 3275.486553095013\n",
      "Current BFE value: 676.8537773787698\n",
      "Current BFE value: 637.9744430974015\n",
      "Current BFE value: 637.974374505051\n",
      "Current BFE value: 637.9743745049896\n",
      "Current BFE value: 637.9743745049882\n",
      "Current BFE value: 637.974374504985\n",
      "Current BFE value: 637.9743745049841\n",
      "Current BFE value: 637.9743745049841\n"
     ]
    }
   ],
   "source": [
    "# Reset the model with vague marginals\n",
    "setmarginal!(μ, vague(NormalMeanPrecision))\n",
    "setmarginal!(τ, vague(GammaShapeRate))\n",
    "\n",
    "for i in 1:10\n",
    "    update!(y, dataset)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It always necessary to unsubscribe and release computer resources\n",
    "unsubscribe!([ μ_subscription, τ_subscription, bfe_subscription ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meta specification\n",
    "\n",
    "During model specification some functional dependencies may accept an optional `meta` object in `where { ... }` clause. The purpose of the `meta` object is to adjust, modify or supply some extra information to the inference backend during messages computations. `meta` object for example may contain an approximation method that needs to be used during various approximations or it may specify the tradeoff between accuracy and performance:\n",
    "\n",
    "```julia\n",
    "# In this example `meta` object for autoregressive `AR` node specifes the variate type of \n",
    "# the autoregressive process and its order. In addition it specifies that messages computation rules \n",
    "# respect accuracy over speed with `ARsafe()` strategy. In contrast, `ARunsafe()` strategy tries to speedup computations\n",
    "# by cost of possible numerical instabilities during an inference procedure\n",
    "s[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Multivariate, order, ARsafe()) }\n",
    "...\n",
    "s[i] ~ AR(s[i - 1], θ, γ) where { q = q(s[i - 1], s[i])q(θ)q(γ), meta = ARMeta(Univariate, order, ARunsafe()) }\n",
    "```\n",
    "\n",
    "Another example with `GaussianControlledVariance`, or simply `GCV` [see Hierarchical Gaussian Filter], node:\n",
    "\n",
    "```julia\n",
    "# In this example we specify structured factorisation and flag meta with `GaussHermiteCubature` \n",
    "# method with `21` sigma points for approximation of non-lineariety between hierarchy layers\n",
    "xt ~ GCV(xt_min, zt, real_k, real_w) where { q = q(xt, xt_min)q(zt)q(κ)q(ω), meta = GCVMetadata(GaussHermiteCubature(21)) }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meta object is usefull to pass any extra information to a node that is not a random variable or constant model variable. It may include extra approximation methods, differentiation methods, optional non-linear functions, extra inference parameters etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating custom nodes and message computation rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom nodes\n",
    "\n",
    "To create a custom functional form and to make it available during model specification `ReactiveMP.jl` exports the `@node` macro:\n",
    "\n",
    "```julia\n",
    "# `@node` macro accepts a name of the functional form, its type, either `Stochastic` or `Deterministic` and an array of interfaces:\n",
    "@node NormalMeanVariance Stochastic [ out, μ, v ]\n",
    "\n",
    "# Interfaces may have aliases for their names that might be convenient for factorisation constraints specification\n",
    "@node NormalMeanVariance Stochastic [ out, (μ, aliases = [ mean ]), (v, aliases = [ var ]) ]\n",
    "\n",
    "# `NormalMeanVariance` structure declaration must exist, otherwise `@node` macro will throw an error\n",
    "struct NormalMeanVariance end \n",
    "\n",
    "@node NormalMeanVariance Stochastic [ out, μ, v ]\n",
    "\n",
    "# It is also possible to use function objects as a node functional form\n",
    "function dot end\n",
    "\n",
    "# Syntax for functions is a bit differet, as it is necesssary to use `typeof(...)` function for them \n",
    "# out = dot(x, a)\n",
    "@node typeof(dot) Deterministic [ out, x, a ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: Deterministic nodes do not support factorisation constraints with `where { q = ... }` clause.\n",
    "\n",
    "After that it is possible to use newly during model specification:\n",
    "\n",
    "```julia\n",
    "@model function test_model()\n",
    "    ...\n",
    "    y ~ dot(x, a)\n",
    "    ...\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom messages computation rules\n",
    "\n",
    "`ReactiveMP.jl` exports `@rule` macro to create custom messages computation rules. For example let us create a simple `+` node to be available for usage in the model specification usage. We refer to *A Factor Graph Approach to Signal Modelling , System Identification and Filtering* [ Sascha Korl, 2005, page 32 ] for a rigorous explanation `+` node in factor graphs. According to Korl, assuming that inputs are Gaussian Sum-Product message computation rule for `+` node is the following:\n",
    "\n",
    "$$\n",
    "\\mu_z = \\mu_x + \\mu_y \\\\\n",
    "V_z = V_x + V_y\n",
    "$$\n",
    "\n",
    "To specify this in `ReactiveMP.jl` we use `@node` and `@rule` macros:\n",
    " \n",
    "```julia\n",
    "@node typeof(+) Deterministic  [ z, x, y ]\n",
    "\n",
    "@rule typeof(+)(:z, Marginalisation) (m_x::UnivariateNormalDistributionsFamily, m_y::UnivariateNormalDistributionsFamily) = begin\n",
    "    x_mean, x_var = mean_var(m_x)\n",
    "    y_mean, y_var = mean_var(m_y)\n",
    "    return NormalMeanVariance(x_mean + y_mean, x_var + y_var)\n",
    "end\n",
    "```\n",
    "\n",
    "In this example, for the `@rule` macro, we specify a type of our functional form: `typeof(+)`. Next, we specify an edge we are going to compute an outbound message for. `Marginalisation` indicates that the corresponding message respects the marginalisation constraint for posterior over corresponding edge:\n",
    "\n",
    "$$\n",
    "q(z) = \\int q(z, x, y) \\mathrm{d}x\\mathrm{d}y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look on difference between sum-product rules and variational rules with mean-field assumption we notice that they require different local information to compute an outgoing message:\n",
    "\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 30%;\" src=\"./pics/sp.png\" width=\"20%\" />\n",
    "\n",
    "$$\n",
    "\\mu(z) = \\int f(x, y, z)\\mu(x)\\mu(y)\\mathrm{d}x\\mathrm{d}y\n",
    "$$\n",
    "\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 30%;\" src=\"./pics/vmp.png\" width=\"20%\" />\n",
    "\n",
    "$$\n",
    "\\nu(z) = \\exp{ \\int \\log f(x, y, z)q(x)q(y)\\mathrm{d}x\\mathrm{d}y }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@rule` macro support both cases with special prefixes during rule specification:\n",
    "- `m_` prefix corresponds to the incoming message on a specific edge\n",
    "- `q_` prefix corresponds to the posterior marginal of a specific edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a Sum-Product rule with `m_` messages used:\n",
    "\n",
    "```julia\n",
    "@rule NormalMeanPrecision(:μ, Marginalisation) (m_out::UnivariateNormalDistributionsFamily, m_τ::PointMass) = begin \n",
    "    m_out_mean, m_out_cov = mean_cov(m_out)\n",
    "    return NormalMeanPrecision(m_out_mean, inv(m_out_cov + inv(mean(m_τ))))\n",
    "end\n",
    "```\n",
    "\n",
    "Example of a Variational rule with Mean-Field assumption with `q_` posteriors used:\n",
    "\n",
    "```julia\n",
    "@rule NormalMeanPrecision(:μ, Marginalisation) (q_out::Any, q_τ::Any) = begin \n",
    "    return NormalMeanPrecision(mean(q_out), mean(q_τ))\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReactiveMP.jl` also supports structured rules. It is possible to obtain joint marginal over a set of edges:\n",
    "\n",
    "```julia\n",
    "@rule NormalMeanPrecision(:τ, Marginalisation) (q_out_μ::Any, ) = begin\n",
    "    m, V = mean_cov(q_out_μ)\n",
    "    θ = 2 / (V[1,1] - V[1,2] - V[2,1] + V[2,2] + abs2(m[1] - m[2]))\n",
    "    α = convert(typeof(θ), 1.5)\n",
    "    return Gamma(α, θ)\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: In `@rule`specification messages or marginals arguments **must** be in order with interfaces specification from `@node` macro:\n",
    "\n",
    "```julia\n",
    "# Inference backend expects arguments in `@rule` macro to be in the same order\n",
    "@node NormalMeanPrecision Stochastic [ out, μ, τ ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any rule always has an access to meta information with hidden `meta::Any` variable:\n",
    "\n",
    "```julia\n",
    "@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any) = begin \n",
    "    ...\n",
    "    println(meta)\n",
    "    ...\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to dispatch on a specific type of a meta object:\n",
    "\n",
    "```julia\n",
    "@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::LaplaceApproximation) = begin \n",
    "    ...\n",
    "end\n",
    "```\n",
    "\n",
    "or\n",
    "\n",
    "```julia\n",
    "@rule MyCustomNode(:out, Marginalisation) (m_in1::Any, m_in2::Any, meta::GaussHermiteCubature) = begin \n",
    "    ...\n",
    "end\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing messages computational pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In certain situation it might be convenient to customize default messages computational pipeline. `GrahpPPL.jl` supports `pipeline` keyword in `where { ... }` clause to add some extra steps after a message has been computed. A use case might be an extra approximation method to preserve conjugacy in the model, debugging or simple printing.\n",
    "\n",
    "<img style=\"display: block;\n",
    "  margin-left: auto;\n",
    "  margin-right: auto;\n",
    "  width: 30%;\" src=\"./pics/pipeline.png\" width=\"20%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LoggerPipelineStage() }\n",
    "y[i] ~ NormalMeanPrecision(x[i], 1.0) where { pipeline = LaplaceApproximation() }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us return to the coin toss model, but this time we want to print flowing messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coin_toss_model_log (generic function with 1 method)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@model function coin_toss_model_log(n)\n",
    "\n",
    "    y = datavar(Float64, n)\n",
    "\n",
    "    θ ~ Beta(2.0, 7.0) where { pipeline = LoggerPipelineStage(\"θ\") }\n",
    "\n",
    "    for i in 1:n\n",
    "        y[i] ~ Bernoulli(θ)  where { pipeline = LoggerPipelineStage(\"y[$i]\") }\n",
    "    end\n",
    "    \n",
    "    return y, θ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (y, θ) = coin_toss_model_log(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[θ][Beta][out]: Message(Beta{Float64}(α=2.0, β=7.0))\n"
     ]
    }
   ],
   "source": [
    "θ_subscription = subscribe!(getmarginal(θ), (value) -> println(\"New posterior marginal for θ: \", value));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "coinflips = float.(rand(Bernoulli(0.5), 5));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[y[1]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))\n",
      "[y[2]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))\n",
      "[y[3]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))\n",
      "[y[4]][Bernoulli][p]: Message(Beta{Float64}(α=1.0, β=2.0))\n",
      "[y[5]][Bernoulli][p]: Message(Beta{Float64}(α=2.0, β=1.0))\n",
      "New posterior marginal for θ: Marginal(Beta{Float64}(α=3.0, β=11.0))\n"
     ]
    }
   ],
   "source": [
    "update!(y, coinflips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsubscribe!(θ_subscription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference is lazy and does not send messages if no one is listening for them\n",
    "update!(y, coinflips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customizing posterior computational pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.2",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
